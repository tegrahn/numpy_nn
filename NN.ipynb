{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa11753-b7e8-45e7-9260-45f4107d9d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class nn:\n",
    "    def __init__(self):\n",
    "        self.bias_weights = []\n",
    "        self.layer_weights = []\n",
    "        self.activation_funcs = []\n",
    "        self.backward_activation_funcs = []\n",
    "        self.learning_rate = 0.1\n",
    "        self.train_costs = []\n",
    "        self.test_costs = []\n",
    "        self.train_accuracies = []\n",
    "        self.test_accuracies = []\n",
    "\n",
    "    def initialise_weights(self, input_size, nodes):\n",
    "        \"\"\"\n",
    "        Set up our weight matrices (bias and layers)\n",
    "        param input_size: Number of features in our input data \n",
    "        param nodes: A list of integers that determines the number of layers and nodes in each\n",
    "        \"\"\"\n",
    "        nodes.insert(0, input_size)\n",
    "        # See paper 1 for how weights are indexed\n",
    "        self.layer_weights=[]\n",
    "        self.bias_weights=[]\n",
    "        # Create layers\n",
    "        for i in range(1, len(nodes)):\n",
    "            self.layer_weights.append(np.random.normal(0,0.1,size=(nodes[i-1],nodes[i])))\n",
    "            self.bias_weights.append(np.zeros((1, nodes[i])))\n",
    "\n",
    "    def initialise_activations(self, activation_func_names):\n",
    "        \"\"\"\n",
    "        Set up activation functions for use in forward and backward pass\n",
    "        param activation_func_names: Which activation functions as a list of strings\n",
    "        \"\"\"\n",
    "        for name in activation_func_names:\n",
    "            if name == 'relu':\n",
    "                self.activation_funcs.append(relu)\n",
    "                self.backward_activation_funcs.append(backward_relu)\n",
    "            elif name == 'sigmoid':\n",
    "                self.activation_funcs.append(sigmoid)\n",
    "                self.backward_activation_funcs.append(sigmoid_backward)\n",
    "            else:\n",
    "                raise Exception(\"Activation function is not implemented. Must be either relu or sigmoid.\")\n",
    "    \n",
    "    def use_weights(self, x):\n",
    "        \"\"\"\n",
    "        Use the current weights to generate the output of the last layer without any other functions applied\n",
    "        param x: 2d array of inputs to use the weights on\n",
    "        \"\"\"\n",
    "        num_layers = len(self.layer_weights)\n",
    "        output = x@self.layer_weights[0]\n",
    "        output = self.activation_funcs[0](output)\n",
    "        for i in range(1,num_layers):\n",
    "            output = output@self.layer_weights[i] + self.bias_weights[i]\n",
    "            output = self.activation_funcs[i](output)\n",
    "        return output\n",
    "    \n",
    "    def model_forward(self, x):\n",
    "        \"\"\"\n",
    "        Use the current weights to generate values for each layer (transfer) and activation\n",
    "        param x: 2d array of inputs to use the weights on\n",
    "        return: A list of the transfer function outputs, A list of the activation function outputs\n",
    "        \"\"\"\n",
    "        num_layers = len(self.layer_weights)\n",
    "        transfers = []\n",
    "        activations = []\n",
    "        transfers.append(x@self.layer_weights[0])\n",
    "        activations.append(self.activation_funcs[0](transfers[0]))\n",
    "        for i in range(1,num_layers):\n",
    "            transfers.append((activations[-1]@self.layer_weights[i]) + self.bias_weights[i])\n",
    "            activations.append(self.activation_funcs[i](transfers[-1]))\n",
    "        return transfers, activations\n",
    "\n",
    "    def model_backward(self, x, y, transfers, activations):\n",
    "        \"\"\"\n",
    "        Compute the gradients for each weights (bias and layers) with respect to the cost function (Softmax-Cross entropy)\n",
    "        param x: 2d array of inputs to use the weights on\n",
    "        param y: 2d array of true values\n",
    "        param tansfers: array of outputs from transfer layers in the neural network\n",
    "        param activations: array of outputs from activation layers in neural network\n",
    "        return: List of weight gradients, List of bias gradients\n",
    "        \"\"\"\n",
    "        weight_grads = []\n",
    "        bias_grads = []\n",
    "        \n",
    "        d_s = backward_softmax_cross(activations[-1], y) #Derivative of cross entropy and softmax\n",
    "        d_prev_layer = self.backward_activation_funcs[-1](d_s, transfers[-1]) #Go through last activation\n",
    "        num_layers = len(self.layer_weights)\n",
    "        num_samples = len(y)\n",
    "        \n",
    "        #Go through each layer in the network\n",
    "        for i in reversed(range(0, num_layers)):\n",
    "            if (i==0):\n",
    "                #The final layer so use the inputs\n",
    "                previous_vals = x\n",
    "            else:\n",
    "                #Not the final layer so use the activations from the previous activations\n",
    "                previous_vals = activations[i-1]\n",
    "           # Gradient for the current layer weights, See paper 1 and 2\n",
    "            weight_grad = np.divide((previous_vals.T@d_prev_layer),num_samples)\n",
    "            weight_grads.append(weight_grad)\n",
    "            # Bias for the current layer bias. \n",
    "            # The sum of the gradients of the previous layer as derivative is just 1 and then need to sum previous grads\n",
    "            bias_grad = np.divide(np.sum(d_prev_layer ,axis=0),num_samples)\n",
    "            bias_grads.append(bias_grad)\n",
    "\n",
    "            #If we are not at the last layer we need to compute the gradients to flow backward\n",
    "            if (i!=0):  \n",
    "                # Retrieve the current weights and use them to compute the next set of gradients\n",
    "                # See paper 3\n",
    "                current_layer = self.layer_weights[i]\n",
    "                d_r = d_prev_layer@current_layer.T\n",
    "                d_prev_layer = self.backward_activation_funcs[i-1](d_r, transfers[i-1])\n",
    "        # Return the reversed for easier iterating they are now in the order of the layers forward\n",
    "        weight_grads.reverse()\n",
    "        bias_grads.reverse()\n",
    "        return weight_grads, bias_grads\n",
    "\n",
    "    def update_parameters(self, weight_grads, bias_grads):\n",
    "        \"\"\"\n",
    "        Use the computed gradients and learning rate from model_backward to update the weights\n",
    "        param weight_grads: list of weight gradients \n",
    "        param bias_grads: list of bias gradients\n",
    "        \"\"\"\n",
    "        num_layers = len(self.layer_weights)\n",
    "        for i in range(0, num_layers):  \n",
    "            self.layer_weights[i] -= weight_grads[i] * self.learning_rate\n",
    "            self.bias_weights[i] -= bias_grads[i] * self.learning_rate\n",
    "        \n",
    "    def step(self, x, y):\n",
    "        \"\"\"\n",
    "        Helper function to make an iteration of gradient descent, with a forward pass, backward pass and update\n",
    "        param x: input values for determining gradients\n",
    "        param y: true values for determining gradients\n",
    "        \"\"\"\n",
    "        transfers, activations = self.model_forward(x)\n",
    "        weight_gradients, bias_gradients = self.model_backward(x, y, transfers, activations)\n",
    "        self.update_parameters(weight_gradients, bias_gradients)\n",
    "                        \n",
    "    def train(self, x_train, y_train, nodes, activation_funcs, learning_rate=0.1, epochs=5, batch_size=32, x_test = [], y_test = []):\n",
    "        \"\"\"\n",
    "        Trains the given neural network on the inputted data using mini batching\n",
    "        param x_train: 2d numpy array of training data x values\n",
    "        param y_train: 2d numpy array of true values for training set (one-hot encoded)\n",
    "        param nodes: array of layer nodes (Final layer must match length of one-hot encoded output values)\n",
    "        param activation_funcs: array of strings indicating activation functions that should be used\n",
    "        param learning_rate: gradient multiplier in gradient descent\n",
    "        param epochs: number of times to go through entire x_train set\n",
    "        param batch_size: size of mini-batches\n",
    "        param x_test: 2d numpy array of test data x values\n",
    "        param y_test: 2d numpy array of true values for test set (one-hot encoded)\n",
    "        \"\"\"\n",
    "        self.initialise_weights(len(x_train[0]), nodes)\n",
    "        self.initialise_activations(activation_funcs)\n",
    "        self.learning_rate = learning_rate\n",
    "        epochs_no = 0\n",
    "        iter_no = 0\n",
    "        while epochs>epochs_no:\n",
    "            for x_batch, y_batch in self.batch_generator(x_train,y_train,batch_size):\n",
    "                self.step(x_batch, y_batch)\n",
    "                self.record(x_batch, y_batch, x_test, y_test, iter_no) # This dramatically increases runtime\n",
    "                if self.test_accuracies[-1] >= 0.90:\n",
    "                    break\n",
    "                iter_no += 1\n",
    "            if self.test_accuracies[-1] >= 0.90:\n",
    "                break\n",
    "            epochs_no += 1\n",
    "\n",
    "    def batch_generator(self, X, y, batchsize):\n",
    "        \"\"\"\n",
    "        Generator function to return mini batches of X and Y\n",
    "        param X: One list to create mini batches on\n",
    "        param Y: One list to create mini batches on\n",
    "        return: A mini batch of size batchsize of X, A mini batch of size batchsize of y\n",
    "        \"\"\"\n",
    "        p = np.random.permutation(len(y))\n",
    "        shuffled_X = X[p]\n",
    "        shuffled_Y = y[p]\n",
    "        for start_index in range(0, len(X) - batchsize + 1, batchsize):\n",
    "            batch = slice(start_index, start_index + batchsize)\n",
    "            yield shuffled_X[batch], shuffled_Y[batch]\n",
    "\n",
    "    def record(self,x_train, y_train, x_test, y_test, iter_no):\n",
    "        \"\"\"\n",
    "        Records the train and test cost and accuracy at each iteration and prints based on iteration print\n",
    "        param x_train: Training values used this iteration\n",
    "        param y_train: True values used this iteration\n",
    "        param x_test: Test values\n",
    "        param y_test: True values for test values\n",
    "        param iter_no: Current iteration of steps\n",
    "        \"\"\"\n",
    "        ITERATION_PRINT = 500 \n",
    "        train_cost = self.cost(x_train, y_train)\n",
    "        self.train_costs.append(train_cost)\n",
    "        test_cost = self.cost(x_test, y_test)\n",
    "        self.test_costs.append(test_cost)\n",
    "        train_acc = self.accuracy(x_train, y_train)\n",
    "        self.train_accuracies.append(train_acc)\n",
    "        test_acc = self.accuracy(x_test, y_test)\n",
    "        self.test_accuracies.append(test_acc)\n",
    "        if iter_no%ITERATION_PRINT==0:\n",
    "            print(f\"Iteration {iter_no} - Train {train_cost} - Test {test_cost} - Train {train_acc} - Test {test_acc}\")\n",
    "    \n",
    "    def accuracy(self, x, true_values):\n",
    "        \"\"\"\n",
    "        Makes a prediction for the given values and returns the accuracy of the prediction\n",
    "        param x: values to predict on\n",
    "        param true_values: true values for given values (one-hot encoded)\n",
    "        return: Accuracy of predictions\n",
    "        \"\"\"\n",
    "        predictions = self.predict(x)\n",
    "        return np.mean(true_values == predictions)\n",
    "\n",
    "    def cost(self, x, true_values):\n",
    "        \"\"\"\n",
    "        Makes a forward pass for the given values and returns the cost\n",
    "        param x: values to predict on\n",
    "        param true_values: true values for given values (one-hot encoded)\n",
    "        return: Softmaxed cross-entropy loss of predictions\n",
    "        \"\"\"\n",
    "        output = self.use_weights(x)\n",
    "        return np.mean(softmax_cross_loss_all(output, true_values))\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Makes a forward pass for the given values and takes the maximum as the prediction\n",
    "        param x: values to predict on\n",
    "        return: One-hot encoded prediction of x-value (Predicted class) \n",
    "        \"\"\"\n",
    "        output = self.use_weights(x)\n",
    "        max_indices = np.argmax(output, axis=1)\n",
    "        preds = np.zeros_like(output)\n",
    "        rows = np.arange(preds.shape[0])\n",
    "        preds[rows, max_indices] = 1\n",
    "        return preds\n",
    "\n",
    "\n",
    "def softmax_cross_loss_all(outputs, one_hot_true_labels):\n",
    "    \"\"\"\n",
    "    Computes the softmax and cross entropy of values given their true labels (one-hot encoded)\n",
    "    param outputs: 2d numpy array of values \n",
    "    param one_hot_true_labels: True labels of given outputs (one-hot encoded)\n",
    "    return: Sum of costs for each output\n",
    "    \"\"\"\n",
    "    # True class - log( Sum of exponents of all)\n",
    "    row_max = np.max(outputs, axis=1, keepdims=True)\n",
    "    maxs = np.repeat(row_max, outputs.shape[1], axis=1)\n",
    "    mask = outputs > 0\n",
    "    outputs[mask] -= maxs[mask]\n",
    "    return -np.sum(np.multiply(outputs, one_hot_true_labels), axis=1) + np.log(np.sum(np.exp(outputs), axis=1))\n",
    "\n",
    "def softmax_all(outputs):\n",
    "    \"\"\"\n",
    "    Computes the softmax of values (Numerical stability by subtracting maximums if > 0)\n",
    "    param outputs: 2d numpy array of values \n",
    "    return: 2d numpy array of softmax values\n",
    "    \"\"\"\n",
    "    row_max = np.max(outputs, axis=1, keepdims=True)\n",
    "    maxs = np.repeat(row_max, outputs.shape[1], axis=1)\n",
    "    mask = outputs > 0\n",
    "    outputs[mask] -= maxs[mask]\n",
    "    exps = np.exp(outputs)\n",
    "    sum_exps = np.sum(exps, axis=1, keepdims=True)\n",
    "    return exps / sum_exps\n",
    "                                                        \n",
    "def backward_softmax_cross(outputs, one_hot_true_labels):\n",
    "    \"\"\"\n",
    "    Computes derivative of the joint softmax and cross entropy function\n",
    "    param outputs: 2d numpy array of values \n",
    "    param one_hot_true_labels: 2d array of true labels (one-hot encoded)\n",
    "    return: 2d numpy array of derivatives\n",
    "    \"\"\"\n",
    "    num_samples = one_hot_true_labels.shape[0]\n",
    "    softmaxes = softmax_all(outputs)\n",
    "    return (-one_hot_true_labels + softmaxes) / num_samples\n",
    "\n",
    "def relu(inputs):\n",
    "    \"\"\"\n",
    "    Applies the RELU function\n",
    "    param inputs: 2d numpy array of values \n",
    "    return: 2d numpy array of values with RELU applied\n",
    "    \"\"\"\n",
    "    return np.maximum(inputs,0)\n",
    "\n",
    "def backward_relu(grads, inputs):\n",
    "    \"\"\"\n",
    "    Computes the derivative of the RELU function\n",
    "    param grads: 2d numpy array of values flowing backward through the RELU \n",
    "    param inputs: 2d numpy array of values that were inputs to the RELU function\n",
    "    return: 2d numpy array of derivative of RELU function\n",
    "    \"\"\"\n",
    "    grads[inputs <= 0] = 0;\n",
    "    return grads;\n",
    "\n",
    "def sigmoid(inputs):\n",
    "    \"\"\"\n",
    "    Applies the sigmoid function\n",
    "    param inputs: 2d numpy array of values \n",
    "    return: 2d numpy array of values with sigmoid applied\n",
    "    \"\"\"\n",
    "    return np.divide(1,(1+np.exp(-inputs)))\n",
    "\n",
    "def sigmoid_backward(grads, inputs):\n",
    "    \"\"\"\n",
    "    Computes the derivative of the sigmoid function\n",
    "    param grads: 2d numpy array of values flowing backward through the sigmoid \n",
    "    param inputs: 2d numpy array of values flowing backward through the sigmoid \n",
    "    return: 2d numpy array of derivative of sigmoid function\n",
    "    \"\"\"\n",
    "    sigmoids = sigmoid(inputs)\n",
    "    return sigmoids*(1-sigmoids)*grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2f6f4f-9f57-45df-ada9-e76cbdccbc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "def training_curve_plot(title, train_losses, test_losses, train_accuracy, test_accuracy):\n",
    "    \"\"\" convenience function for plotting train and test loss and accuracy\n",
    "    \"\"\"\n",
    "    lg=13\n",
    "    md=10\n",
    "    sm=9\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    fig.suptitle(title, fontsize=lg)\n",
    "    x = range(1, len(train_losses)+1)\n",
    "    axs[0].plot(x, test_losses, label=f'Final test loss: {test_losses[-1]:.4f}')\n",
    "    axs[0].plot(x, train_losses, label=f'Final train loss: {train_losses[-1]:.4f}')\n",
    "    axs[0].set_title('Losses', fontsize=md)\n",
    "    axs[0].set_xlabel('Iteration', fontsize=md)\n",
    "    axs[0].set_ylabel('Loss', fontsize=md)\n",
    "    axs[0].legend(fontsize=sm)\n",
    "    axs[0].tick_params(axis='both', labelsize=sm)\n",
    "    # Optionally use a logarithmic y-scale\n",
    "    #axs[0].set_yscale('log')\n",
    "    axs[0].grid(True, which=\"both\", linestyle='--', linewidth=0.5)\n",
    "    axs[1].plot(x, 100*train_accuracy, label=f'Final train accuracy: {100*train_accuracy[-1]:.4f}%')\n",
    "    axs[1].plot(x, 100*test_accuracy, label=f'Final test accuracy: {100*test_accuracy[-1]:.4f}%')\n",
    "    axs[1].set_title('Accuracy', fontsize=md)\n",
    "    axs[1].set_xlabel('Iteration', fontsize=md)\n",
    "    axs[1].set_ylabel('Accuracy (%)', fontsize=sm)\n",
    "    axs[1].legend(fontsize=sm)\n",
    "    axs[1].tick_params(axis='both', labelsize=sm)\n",
    "    axs[1].grid(True, which=\"both\", linestyle='--', linewidth=0.5)\n",
    "    plt.savefig('loss_acc.png')\n",
    "    plt.show()\n",
    "\n",
    "def plot_weights(weights):\n",
    "    fig, axs = plt.subplots(2, 5, figsize=(10, 4))\n",
    "    for i, ax in enumerate(axs.flat):\n",
    "        ax.imshow(weights[i], cmap='Greys')\n",
    "    fig.suptitle('Single layer neural network weights after training')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('weights.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17a1e80-0e39-429e-baad-eb0147e9b761",
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_mnist import load_mnist\n",
    "X_train, Y_train, X_test, Y_test = load_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a59da86-6f0c-406c-b431-5b0bc4ba184b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "the_nn = nn()\n",
    "start_time = time.time()\n",
    "the_nn.train(X_train, Y_train, [111, 10], ['relu','relu'], learning_rate=0.4, epochs=80, batch_size=50, x_test = X_test, y_test = Y_test)\n",
    "training_time = time.time() - start_time\n",
    "print(training_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0c2b3a-c166-46d5-bc91-59636828f6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_curve_plot(\"Multi-layer neural network training\", np.array(the_nn.test_costs), np.array(the_nn.train_costs), np.array(the_nn.train_accuracies), np.array(the_nn.test_accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f18689-0b64-4642-944d-69ee2b079d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "the_nn_1 = nn()\n",
    "the_nn_1.train(X_train, Y_train, [10], ['relu'], learning_rate=0.5, epochs=1, batch_size=40, x_test = X_test, y_test = Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8a5de8-9054-4db9-8ec6-ae7f24f95daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_curve_plot(\"Single layer nn\", np.array(the_nn_1.test_costs), np.array(the_nn_1.train_costs), np.array(the_nn_1.train_accuracies), np.array(the_nn_1.test_accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7ed815-6575-4f9f-a4bf-f44d3f1f9d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = the_nn_1.layer_weights[-1].T.reshape(10,28,28)\n",
    "plot_weights(weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
